# YouTube Video Script & Talking Points

**Video Title**: *"Building Voice AI Agents: 3 Architectures Compared (Vercel AI SDK vs LiveKit)"*

**Duration**: 12-15 minutes  
**Target Audience**: Developers interested in AI agents, voice interfaces, and real-time applications

---

## ğŸ¬ Video Structure

### 1. Hook (0:00 - 0:30)
**Visual**: Quick demo of all 3 approaches side-by-side  
**Script**:
> "I built the same voice ordering agent three different ways. One uses Vercel's AI SDK, another uses LiveKit's client SDK, and the third uses LiveKit's native agents. The results? Completely different architectures with wildly different performance. Let me show you which one you should use."

**On-screen text**:
- ğŸ¯ Approach 1: 600ms latency
- ğŸ¯ Approach 2: 800ms latency
- ğŸ¯ Approach 3: 400ms latency

---

### 2. Introduction (0:30 - 2:00)
**Visual**: Speaking head + code editor  
**Script**:
> "Hey everyone! Today we're diving deep into voice-enabled AI agents. I'm going to show you three completely different ways to build the same application - a voice ordering system for a food delivery app.
>
> We'll be using two amazing technologies: Vercel's AI SDK and LiveKit. Both are production-ready, but they take very different approaches to solving this problem.
>
> By the end of this video, you'll understand the tradeoffs between each approach and know exactly which one to use for your next project."

**B-roll suggestions**:
- Code editor showing project structure
- Browser with all three pages open
- Terminal showing servers running
- Quick animation of the architectures

**On-screen tags**:
- @vercel
- @livekit
- #VoiceAI
- #AgenticAI

---

### 3. Project Demo (2:00 - 3:30)
**Visual**: Full screen recording of the app  
**Script**:
> "Let me show you what we're building. This is a food ordering application where users can place orders using their voice. Watch what happens when I say 'I want Thai food for lunch.'"

**Demo actions**:
1. Navigate to home page
2. Click into any concierge approach
3. Click "Start Conversation"
4. Say: "I want Thai food for lunch"
5. Show cards rendering
6. Say: "Add the pad thai to my cart"
7. Show cart modal update
8. Say: "View my cart"
9. Say: "Submit my order"
10. Show order confirmation

**Key points to highlight**:
- Cards render automatically during conversation
- Cart updates in real-time
- Voice and visual UI work together
- Natural conversation flow

---

### 4. Architecture 1: AI SDK Approach (3:30 - 6:00)

#### Overview (3:30 - 4:00)
**Visual**: Code editor showing `/app/food/concierge/page.tsx`  
**Script**:
> "Let's start with the simplest approach: Vercel's AI SDK. This is what most developers should reach for first, and here's why."

**On-screen**: Architecture diagram (refer to VISUAL_DIAGRAMS.md)

#### Code Walkthrough (4:00 - 5:00)
**Visual**: Split screen - code on left, running app on right  
**Script**:
> "The frontend is incredibly simple. We just use the `useChat` hook:"

```typescript
const { messages, input, handleSubmit } = useChat({
  api: '/api/chat',
  body: { restaurantId: selectedRestaurant?.id }
});
```

> "That's it. The hook handles all state management, streaming, and even optimistic updates. Now the backend:"

```typescript
export async function POST(req: Request) {
  const result = streamText({
    model: openai('gpt-4o'),
    system: SYSTEM_PROMPT,
    messages,
    tools: {
      addItemToCart: tool({ /* ... */ }),
      viewCart: tool({ /* ... */ })
    }
  });
  
  return result.toDataStreamResponse();
}
```

> "Look at this. The AI SDK automatically handles tool execution. When the LLM wants to add an item to the cart, the SDK calls our function, waits for the result, and sends it back to the LLM. All automatic."

**Highlight in video**:
- Point to `useChat` hook
- Point to `streamText` function
- Point to automatic tool execution
- Show SSE stream in Network tab

#### Pros & Cons (5:00 - 5:30)
**Visual**: Animated list  
**Script**:
> "So why choose this approach?"

**Show on screen**:
âœ… **Pros:**
- Minimal code (~50 lines)
- TypeScript end-to-end
- Automatic streaming
- Built-in tool execution
- Perfect for chat-first apps

âŒ **Cons:**
- Higher latency (~600ms)
- Browser-based voice (varies)
- Not optimized for voice
- No interruption handling

#### When to Use (5:30 - 6:00)
**Script**:
> "Use this approach when you're building a chat interface where voice is optional. It's perfect for chatbots, support agents, or any text-first application where you might add voice later. The developer experience is fantastic, and you can ship incredibly fast."

---

### 5. Architecture 2: Manual LiveKit (6:00 - 8:30)

#### Overview (6:00 - 6:30)
**Visual**: Code editor showing `/app/food/concierge-livekit/page.tsx`  
**Script**:
> "The second approach uses LiveKit's Client SDK, but with manual SSE parsing. This gives us full control over the data flow. It's more complex, but also more flexible."

**On-screen**: Manual LiveKit architecture diagram

#### Code Walkthrough (6:30 - 7:30)
**Visual**: Split screen showing frontend and backend  
**Script**:
> "First, we connect to a LiveKit room:"

```typescript
const room = new Room();
await room.connect(livekitUrl, token);
```

> "Then we manually fetch and parse Server-Sent Events:"

```typescript
const response = await fetch('/api/voice-chat', {
  method: 'POST',
  body: JSON.stringify({ messages })
});

const reader = response.body.getReader();
while (true) {
  const { value, done } = await reader.read();
  if (done) break;
  
  const chunk = new TextDecoder().decode(value);
  // Manual parsing of SSE format
  if (line.startsWith('data: ')) {
    const data = JSON.parse(line.slice(6));
    if (data.type === 'text-delta') {
      // Handle streaming text
    }
    if (data.type === 'tool-output-available') {
      // Render cards manually
    }
  }
}
```

> "Notice we're manually parsing every chunk, manually detecting tool calls, and manually rendering UI. This is powerful but verbose."

**Highlight in video**:
- Point to manual while loop
- Point to manual JSON parsing
- Point to conditional logic for different event types
- Show Network tab with SSE stream
- Show LiveKit room in DevTools

#### Backend Deep Dive (7:30 - 8:00)
**Visual**: Backend code  
**Script**:
> "On the backend, we're creating our own SSE stream:"

```typescript
const stream = new ReadableStream({
  async start(controller) {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages,
      tools: TOOLS_SCHEMA,
      stream: true
    });
    
    for await (const chunk of response) {
      // Manually encode SSE events
      if (delta.tool_calls) {
        const result = await executeToolCall(delta.tool_calls[0]);
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify({
            type: 'tool-output-available',
            result: result
          })}\n\n`)
        );
      }
    }
  }
});
```

> "We're manually handling every aspect of the stream. This means we can customize the format, add custom events, or integrate with other systems."

#### Pros & Cons (8:00 - 8:30)
**Visual**: Animated list  
**Script**:
> "So when would you use this?"

**Show on screen**:
âœ… **Pros:**
- Full control over streaming
- Custom SSE event format
- LiveKit infrastructure
- TypeScript end-to-end
- Flexible for complex workflows

âŒ **Cons:**
- Most code (~300 lines)
- Manual error handling
- Manual parsing required
- Still ~800ms latency
- No automatic interruption

> "Choose this when you need custom control over your data flow, or when you're integrating multiple real-time features beyond just voice."

---

### 6. Architecture 3: Native LiveKit Agents (8:30 - 11:30)

#### Overview (8:30 - 9:00)
**Visual**: Terminal showing Python agent starting  
**Script**:
> "Now for the game-changer: LiveKit's native Agents SDK. This is a completely different approach - we're running a Python agent that handles everything automatically. And the results are impressive."

**On-screen**: Native architecture diagram with animation

**Show terminal output**:
```
INFO     livekit.agents: Agent started: A_oNWqNCBgiXed
INFO     livekit.agents: Joined room: RM_CCoRjfCxxdYb
INFO     livekit.agents: Voice pipeline ready (400ms latency)
```

#### The Magic of voice.Agent (9:00 - 10:00)
**Visual**: Python code in editor  
**Script**:
> "Look at this Python code. This is all it takes to create a production-ready voice agent:"

```python
async def entrypoint(ctx: JobContext):
    agent = voice.Agent(
        vad=silero.VAD.load(),         # Voice activity detection
        stt=openai.STT(),               # Speech-to-text
        llm=openai.LLM(model="gpt-4o"), # Language model
        tts=openai.TTS(),               # Text-to-speech
        chat_ctx=llm.ChatContext().append(
            text=SYSTEM_PROMPT,
            role="system"
        )
    )
    
    await agent.start(ctx.room)
```

> "That's it. Those few lines give us automatic speech-to-text, LLM processing, and text-to-speech. The agent automatically joins rooms, handles voice activity detection, and even handles interruptions when users cut it off mid-sentence."

**Highlight**:
- Point to each component (VAD, STT, LLM, TTS)
- Emphasize "automatic"
- Show terminal logs of agent processing

#### Tool Functions (10:00 - 10:45)
**Visual**: Python tool function code  
**Script**:
> "Here's where it gets really elegant. To add a tool, we just use a decorator:"

```python
@llm.ai_callable()
async def add_item_to_cart(
    item_id: int,
    quantity: int,
    special_instructions: str = ""
):
    """Add an item to the user's cart"""
    
    # Regular Python function
    async with get_db_connection() as conn:
        cart = await add_to_cart(conn, user_id, item_id, quantity)
    
    # Send data to frontend
    await ctx.room.local_participant.publish_data(
        json.dumps({"type": "cart_updated", "cart": cart})
    )
    
    return f"Added {quantity}x {item_name} to your cart"
```

> "The decorator tells the LLM this function is available. When the model wants to add something to the cart, it just calls this Python function. No manual parsing, no conditional logic, just a regular Python function that gets called automatically."

**Highlight**:
- Point to `@llm.ai_callable()` decorator
- Point to docstring (LLM sees this)
- Point to type hints (automatic schema generation)
- Show frontend receiving the data message

#### Performance Comparison (10:45 - 11:15)
**Visual**: Split screen with all 3 approaches processing same request  
**Script**:
> "Now let's talk performance. I tested all three approaches saying the same thing: 'Add pad thai to my cart.' Watch the difference."

**Show timer on screen for each**:
- AI SDK: Timer shows ~600ms
- Manual LiveKit: Timer shows ~800ms
- Native Agents: Timer shows ~400ms

> "The native agent is 33% faster than the AI SDK approach and 50% faster than manual LiveKit. That's because everything happens server-side - no browser overhead, no API roundtrips, just LiveKit's optimized voice pipeline."

**Show side-by-side visual comparison**

#### Pros & Cons (11:15 - 11:30)
**Visual**: Animated list  
**Script**:
> "So what's the trade-off?"

**Show on screen**:
âœ… **Pros:**
- **Lowest latency** (~400ms)
- Automatic STTâ†’LLMâ†’TTS
- Built-in interruption handling
- Voice activity detection
- Production-ready
- Clean decorator pattern

âŒ **Cons:**
- Requires Python
- Two processes (Next.js + Python)
- Additional deployment
- Learning curve for Agents SDK

> "This is hands-down the best approach for voice-first applications. If your users are primarily speaking, not typing, this is what you want."

---

### 7. Comparison & Recommendations (11:30 - 12:30)

#### Decision Matrix (11:30 - 12:00)
**Visual**: Comparison table with checkmarks  
**Script**:
> "So which should you choose? Here's my recommendation:"

**Show table**:
| If you're building... | Use this |
|----------------------|----------|
| Chat-first interface | âœ… AI SDK |
| Voice is optional | âœ… AI SDK |
| Team only knows TypeScript | âœ… AI SDK or Manual |
| Voice-first application | âœ… Native Agents |
| Latency is critical | âœ… Native Agents |
| Need custom streaming | âœ… Manual LiveKit |
| Complex integration | âœ… Manual LiveKit |

> "Most developers should start with Vercel's AI SDK. It's simple, powerful, and perfect for 80% of use cases. Only move to the other approaches when you have specific needs."

#### Technology Attribution (12:00 - 12:30)
**Visual**: Split screen with Vercel and LiveKit logos  
**Script**:
> "I want to give huge shout-outs to both Vercel and LiveKit. Vercel's AI SDK has completely changed how I build LLM applications. The developer experience is incredible, and the automatic tool handling is magic.
>
> LiveKit's Agents SDK is equally impressive. Building production voice infrastructure used to require months of work. Now it's just a few lines of Python. The latency improvements and automatic pipeline management are game-changing.
>
> Both teams are building amazing developer tools, and I'm excited to see where they go next."

**On-screen tags**:
- @vercel - AI SDK
- @livekit - Agents SDK & Infrastructure
- Links to their docs

---

### 8. Live Demo (12:30 - 14:00)
**Visual**: Full-screen app demo  
**Script**:
> "Let me show you one more time how all this works in practice. I'm going to place a complete order using voice."

**Demo script**:
1. "I want to see Thai restaurants"
2. [Cards appear]
3. "Add the pad thai with extra spice to my cart"
4. [Cart updates, card shows]
5. "Also add the spring rolls"
6. [Another card appears]
7. "Actually, make that two orders of spring rolls"
8. [Cart updates]
9. "What's in my cart?"
10. [Cart modal opens]
11. "Submit my order"
12. [Order confirmation]

**Highlight during demo**:
- Cards appearing instantly
- Cart updating in real-time
- Natural conversation flow
- Tolerance for variations ("also add", "actually make that")
- Order confirmation with details

---

### 9. Code Availability & Resources (14:00 - 14:30)
**Visual**: GitHub repo  
**Script**:
> "All the code for this project is available on GitHub. You'll find:
> - Complete implementations of all three approaches
> - Database setup scripts
> - Test files for validation
> - Comprehensive documentation
> - Deployment guides
>
> Links are in the description. If you found this helpful, give the repo a star!"

**Show on screen**:
- GitHub repo link
- Documentation links
- Vercel AI SDK docs
- LiveKit docs
- Your social media

---

### 10. Call to Action (14:30 - 15:00)
**Visual**: Speaking head  
**Script**:
> "Thanks for watching! If you enjoyed this deep dive, hit that like button and subscribe for more content about AI agents, real-time applications, and modern web development.
>
> Leave a comment if you've built voice agents before - I'd love to hear about your experiences and which approach you prefer.
>
> In my next video, I'll show you how to deploy all three of these approaches to production. See you then!"

**End screen**:
- Subscribe button animation
- Links to related videos
- GitHub repo (prominent)
- Social media handles

---

## ğŸ“ B-Roll Ideas

### Code Snippets to Show:
1. `useChat` hook initialization
2. `streamText` with tools
3. Manual SSE parsing loop
4. `@llm.ai_callable()` decorator
5. LiveKit room connection
6. Python agent startup

### Terminal Recordings:
1. `npm run dev` starting Next.js
2. `python agents/food_concierge_native.py dev` starting agent
3. Agent logs showing room join
4. Test scripts running successfully

### UI Elements:
1. Card animations appearing
2. Cart modal opening/closing
3. Order history updating
4. Network tab showing SSE streams
5. Browser DevTools showing LiveKit connection

### Diagrams:
Use the Mermaid diagrams from VISUAL_DIAGRAMS.md:
1. Architecture comparison (side-by-side)
2. Sequence diagrams for each approach
3. Latency comparison chart
4. Decision tree

---

## ğŸ¯ Key Messages to Reinforce

### Throughout Video:
1. **Three valid approaches** - no single "best" option
2. **Choose based on your needs** - voice-first vs chat-first
3. **Both technologies are excellent** - Vercel and LiveKit
4. **Production-ready** - not just demos
5. **Developer experience matters** - simpler is often better

### Technical Details to Mention:
- SSE (Server-Sent Events) format
- WebRTC for real-time communication
- Function calling / tool execution
- Voice activity detection
- Interruption handling
- Latency measurements (exact numbers)

---

## ğŸ“Š Metrics to Track

After publishing, track:
- Views in first 24 hours
- Average watch time (aim for 60%+)
- Click-through rate to GitHub
- Comments about which approach viewers prefer
- Shares on Twitter/LinkedIn

---

## ğŸ”— Links for Description

```markdown
ğŸ™ï¸ Building Voice AI Agents: Three Approaches Compared

Compare three different architectures for voice-enabled AI agents using 
Vercel AI SDK and LiveKit.

ğŸ”— RESOURCES
â”œâ”€ GitHub Repo: [your-link]
â”œâ”€ Documentation: [your-link]
â”œâ”€ Architecture Guide: [your-link]
â””â”€ Live Demo: [your-link]

ğŸ“š TECHNOLOGIES
â”œâ”€ Vercel AI SDK: https://sdk.vercel.ai/docs
â”œâ”€ LiveKit Agents: https://docs.livekit.io/agents/
â”œâ”€ OpenAI API: https://platform.openai.com/docs
â””â”€ Next.js: https://nextjs.org

â±ï¸ TIMESTAMPS
00:00 - Hook & Introduction
02:00 - Project Demo
03:30 - AI SDK Approach
06:00 - Manual LiveKit
08:30 - Native Agents
11:30 - Comparison & Recommendations
12:30 - Live Demo
14:00 - Code & Resources

ğŸ·ï¸ TAGS
#VoiceAI #AgenticAI #LiveKit #VercelAI #OpenAI #NextJS #Python 
#WebDevelopment #AI #MachineLearning #SoftwareEngineering

ğŸ’¬ Which approach would you use for your next project? Let me know 
in the comments!

ğŸ”” Subscribe for more AI development tutorials and real-time 
app walkthroughs!
```

---

## ğŸ¨ Thumbnail Ideas

### Option 1: Side-by-Side Comparison
- Split into 3 vertical sections
- Each section labeled with approach name
- Latency numbers prominently displayed
- Your face in corner
- Bold text: "Which is BEST?"

### Option 2: Performance Focus
- Large "400ms vs 600ms" text
- Visual of voice waveforms
- LiveKit + Vercel logos
- "Voice AI Showdown"

### Option 3: Code Focus
- Three code snippets side by side
- Highlight the simplicity differences
- "3 Ways to Build Voice Agents"
- Tech logos (Vercel, LiveKit, OpenAI)

**Thumbnail design tips**:
- Use high contrast colors
- Make text readable at small size
- Include your branding
- Show it's a comparison video
- Use emojis sparingly (ğŸ™ï¸ âš¡ ğŸš€)

---

Good luck with your video! ğŸ¬
